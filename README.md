# Prompt Injection Lab (OWASP LLM01)

This repository is a hands-on **prompt injection** lab designed to teach and demonstrate how attackers can hijack Large Language Model (LLM) behavior using crafted prompts (OWASP LLM01: Prompt Injection).

The lab is **purely educational** and focuses on safe, manual testing against LLM-based chatbots that you are allowed to use (public demos, your own apps, etc.). No illegal activity, no real system hacking.

---

## ğŸ¯ Learning goals

By the end of this lab, you will be able to:

- Explain what **Prompt Injection (LLM01)** is and why it is dangerous for LLM apps.  
- Perform **direct prompt injection** attacks (user directly overrides model instructions).  
- Perform **indirect prompt injection** attacks (malicious instructions hidden in external content like emails or documents).  
- Attempt **system prompt leakage** (trying to reveal hidden system/developer instructions).  
- Document findings in a structured, red-team style report.

---

## ğŸ§± Repository structure

```
prompt-injection-lab/
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ labs/
â”‚   â”œâ”€â”€ lab1-direct-injection/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ lab2-indirect-injection/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ lab3-system-prompt-leak/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ lab4-advanced-attacks/
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ payloads/
â”‚   â”œâ”€â”€ direct.txt
â”‚   â”œâ”€â”€ indirect.txt
â”‚   â”œâ”€â”€ leakage.txt
â”‚   â””â”€â”€ advanced.txt
â”œâ”€â”€ solutions/
â”‚   â”œâ”€â”€ lab1-writeup.md
â”‚   â”œâ”€â”€ lab2-writeup.md
â”‚   â””â”€â”€ lab3-writeup.md
â””â”€â”€ references.md
```

---

## ğŸš€ How to use this lab

1. Pick any LLM chatbot you are allowed to test (public web UI, your own local LLM, or a demo app).  
2. Open one of the labs under `labs/` and read the scenario + instructions.  
3. Use the payloads from the `payloads/` folder and paste them into the chatbot.  
4. Observe the behavior and log results in the format shown in `solutions/`.  
5. Optionally, fork this repo and add your own labs, payloads, or writeups.

This lab does **not** provide any backend code; it is modelâ€‘agnostic and can be used with any LLM provider.

---

## ğŸ›¡ï¸ Safety & ethics

- Use this lab only on systems you **own or are explicitly allowed to test**.  
- Do not attempt to exfiltrate real secrets, private data, or abuse production systems.  
- The goal is to learn how to **defend** LLM apps by understanding real attack patterns.  

---

## ğŸ“š Background

This lab is inspired by:

- OWASP LLM Top 10 â€“ **LLM01: Prompt Injection**.  
- Public prompt injection examples and payload collections (e.g., PayloadsAllTheThings).  

See [`references.md`](./references.md) for more reading.

---

## ğŸ›¡ï¸ Defense section

This repository also includes [`DEFENSE.md`](./DEFENSE.md), which summarizes practical defenses against OWASP LLM01 prompt injection:

- Stronger system prompts and clear role definitions  
- Separating instructions from untrusted data  
- Input validation and injection filters  
- Least-privilege design for LLM agents with tools  
- Output monitoring and human-in-the-loop for risky actions  
- Continuous adversarial testing integrated into CI/CD
