# ğŸ§  AI Prompt Injection Lab with Secure Coding
**OWASP LLM01 | Red Team + Blue Team | Production-Ready**

```bash
  ____                          _     ___        _           _   _ 
 |  _ \ _ __ ___  _ __ ___  ___| |_  |_ _|_ __  (_) ___  ___| |_(_) ___  _ __ 
 | |_) | '__/ _ \| '_ ` _ \/ __| __|  | || '_ \ | |/ _ \/ __| __| |/ _ \| '_ \
 |  __/| | | (_) | | | | | \__ \ |_   | || | | || |  __/ (__| |_| | (_) | | | |
 |_|   |_|  \___/|_| |_| |_|___/\__| |___|_| |_|/ |\___|\___|\__|_|\___/|_| |_|
                                              |__/                          
```

[![OWASP LLM01](https://img.shields.io/badge/OWASP-LLM01%3A2025-red)](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Defense](https://img.shields.io/badge/Defense-DEFENSE.md-blue)](./DEFENSE.md)
[![CI](https://img.shields.io/badge/CI-GitHub%20Actions-success)](./.github/workflows/redteam-test.yml)

[![Tests](https://img.shields.io/badge/Tests-Promptfoo-purple)](https://www.promptfoo.dev)

> Project Overview: Understand the attack â†’ measure the risk â†’ design layered defenses â†’ automate security testing

---

## ğŸ“Œ Project Overview

**AI Prompt Injection Lab with Secure Coding** is an end-to-end, hands-on security project focused on understanding, testing, and mitigating **Prompt Injection attacks** against Large Language Models (LLMs).

This project goes beyond just demonstrating attacksâ€”it demonstrates how to build secure, real-world AI applications aligned with **OWASP LLM01 (2025)** standards.

---

## ğŸ¯ Why This Project Exists

Today's GenAI applicationsâ€”**Chatbots, RAG systems, AI agents, Customer support bots**â€”all follow natural language instructions. This characteristic makes them extremely vulnerable to Prompt Injection.

This lab addresses this problem from both **Offensive (Red Team)** and **Defensive (Blue Team)** perspectives.

---

## ğŸ§© What This Project Covers

### ğŸ”´ Offensive Security (Red Team)
- **Direct Prompt Injection**
- **Indirect Prompt Injection** (documents, emails, hidden text)
- **System Prompt Leakage**
- **Instruction hierarchy bypass attempts**

### ğŸ”µ Defensive Security (Blue Team)
- **Secure prompt design**
- **Instruction vs Data separation**
- **Input validation & injection filtering**
- **Least privilege for LLM tools**
- **Output monitoring & policy enforcement**
- **CI/CD based adversarial testing**

---

## ğŸ§ª Labs Breakdown

### ğŸ§ª Lab 1 â€“ Direct Prompt Injection
**Goal:** Override system instructions via user-controlled input.
- **Focus:** "Ignore previous instructions", Role manipulation, Safety bypass attempts.
- **Skill Learned:** Mastering how to break the LLM instruction hierarchy.

### ğŸ§ª Lab 2 â€“ Indirect Prompt Injection
**Goal:** Injection via documents, emails, or hidden content.
- **Focus:** Malicious instructions inside data, Innocent-looking user prompts, Summarization abuse.
- **Skill Learned:** Understanding dangerous RAG/enterprise application risks.

### ğŸ§ª Lab 3 â€“ System Prompt Leakage
**Goal:** Exposing hidden system rules, policies, or configurations.
- **Focus:** Meta-requests, Debug mode tricks, Simulation-based leakage.
- **Skill Learned:** Implications of sensitive AI configuration exposure.

### ğŸ›¡ï¸ Lab 4 â€“ Defense Hardening (Secure Coding)
**Goal:** Hardening a vulnerable chatbot to be secure and production-ready.
- **Includes:** `vulnerable_prompt.py`, `hardened_prompt.py`, Automated injection tests.
- **Skill Learned:** Secure coding for AI systems (Rare skill ğŸ”¥).

---

## ï¿½ Usage Examples

### 1ï¸âƒ£ Run all tests (API Key required)
```bash
./run-tests.sh
```

### 2ï¸âƒ£ Run free local tests (Ollama)
```bash
./run-tests-free.sh
```

### 3ï¸âƒ£ Run a specific lab
```bash
cd labs/lab1-direct-injection
# Follow lab instructions located in the directory
```

---

## ï¿½ğŸ” Secure Coding & Defense Strategy

The project's [`DEFENSE.md`](./DEFENSE.md) outlines real-world mitigation strategies:

### âœ” Secure Prompt Engineering
- Clear role definition.
- Explicit refusal of meta-instructions.
- Instruction priority enforcement.

### âœ” Instruction vs Data Separation
```text
[INSTRUCTIONS] Trusted system rules [/INSTRUCTIONS]
[UNTRUSTED_CONTENT] User / document input [/UNTRUSTED_CONTENT]
```

### âœ” Input Validation (Code-Level)
- Regex-based injection detection.
- Unicode obfuscation removal.
- Blocking system-level instructions from user input.

### âœ” Least Privilege Design
- LLM tools with restricted permissions.
- No direct DB / filesystem access.
- External policy enforcement layer.

### âœ” Output Monitoring
- System prompt leakage detection.
- PII / sensitive data patterns.
- Human-in-the-loop for risky actions.

---

## ğŸ¤– Automated Security Testing (Production Ready)

We use **Promptfoo** integrated with **CI/CD** for continuous security verification.

```bash
./run-tests.sh
```

## ğŸ†“ Free Student Testing (No API key)

Students can run all tests locally using **Ollama + Promptfoo** (no paid API required).

```bash
chmod +x run-tests-free.sh
./run-tests-free.sh
```

Outputs:
- `reports/free-report.html` (shareable report)
- `reports/free-report.json`

**What happens:**
1. 50+ injection payloads tested.
2. Multi-LLM provider support.
3. Pass/Fail metrics generated.
4. HTML security report created.
5. GitHub Actions runs on every push.

**This proves:**  
ğŸ‘‰ Security is continuously enforced, not just manually tested.

---

## ğŸ“Š Measuring Security Effectiveness

| Metric | Target |
|--------|--------|
| Injection Success Rate | < 5% |
| System Prompt Leakage | 0% |
| False Positives | < 2% |
| Detection Time | < 1 hour |

---

## ğŸ§  What This Project Demonstrates

- âœ” **OWASP LLM01 expertise**
- âœ” **AI Red Teaming skills**
- âœ” **Secure coding for LLM apps**
- âœ” **DevSecOps & CI/CD mindset**
- âœ” **Real-world GenAI risk understanding**

*Not just â€œhow to break AIâ€ â€” but how to keep it secure.*

---

## ğŸš€ Who This Project Is For

- **AI Security / LLM Security roles**
- **Red Team / AppSec engineers**
- **SOC analysts working with GenAI**
- **Cybersecurity students moving into AI security**

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸŒŸ Show Your Support

If you like this project, please give it a â­ï¸!

## ğŸ“§ Contact & Support

For any questions, feedback, or support, please open an issue or reach out to the author.

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

---

## âš ï¸ Legal Disclaimer

This project is strictly for:
- **Education**
- **Research**
- **Authorized testing**

âŒ **No misuse.**  
âŒ **No production attacks.**

---

## ğŸ‘¤ Author

**Amresh Kumar**  
Cybersecurity | AI Red Teaming | Secure Coding  
GitHub: [Ak-cybe](https://github.com/Ak-cybe)

---

### ğŸ Final Note
> The best security engineers donâ€™t just find vulnerabilities â€” they design systems that stay secure under attack.
