# üß† AI Prompt Injection Lab with Secure Coding
**OWASP LLM01 | Red Team + Blue Team | Production-Ready**

```bash
  ____                          _     ___        _           _   _ 
 |  _ \ _ __ ___  _ __ ___  ___| |_  |_ _|_ __  (_) ___  ___| |_(_) ___  _ __ 
 | |_) | '__/ _ \| '_ ` _ \/ __| __|  | || '_ \ | |/ _ \/ __| __| |/ _ \| '_ \
 |  __/| | | (_) | | | | | \__ \ |_   | || | | || |  __/ (__| |_| | (_) | | | |
 |_|   |_|  \___/|_| |_| |_|___/\__| |___|_| |_|/ |\___|\___|\__|_|\___/|_| |_|
                                              |__/                          
```

[![OWASP LLM01](https://img.shields.io/badge/OWASP-LLM01%3A2025-red)](https://genai.owasp.org/llmrisk/llm01-prompt-injection/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Defense](https://img.shields.io/badge/Defense-DEFENSE.md-blue)](./DEFENSE.md)
[![CI](https://img.shields.io/badge/CI-GitHub%20Actions-success)](./.github/workflows/redteam-test.yml)

[![Tests](https://img.shields.io/badge/Tests-Promptfoo-purple)](https://www.promptfoo.dev)

> Project Overview: Understand the attack ‚Üí measure the risk ‚Üí design layered defenses ‚Üí automate security testing

---

## üìå Project Overview

**AI Prompt Injection Lab with Secure Coding** is an end-to-end, hands-on security project focused on understanding, testing, and mitigating **Prompt Injection attacks** against Large Language Models (LLMs).

This project goes beyond just demonstrating attacks‚Äîit demonstrates how to build secure, real-world AI applications aligned with **OWASP LLM01 (2025)** standards.

---

## üéØ Why This Project Exists

Today's GenAI applications‚Äî**Chatbots, RAG systems, AI agents, Customer support bots**‚Äîall follow natural language instructions. This characteristic makes them extremely vulnerable to Prompt Injection.

This lab addresses this problem from both **Offensive (Red Team)** and **Defensive (Blue Team)** perspectives.

---

## üß© What This Project Covers

### üî¥ Offensive Security (Red Team)
- **Direct Prompt Injection**
- **Indirect Prompt Injection** (documents, emails, hidden text)
- **System Prompt Leakage**
- **Instruction hierarchy bypass attempts**

### üîµ Defensive Security (Blue Team)
- **Secure prompt design**
- **Instruction vs Data separation**
- **Input validation & injection filtering**
- **Least privilege for LLM tools**
- **Output monitoring & policy enforcement**
- **CI/CD based adversarial testing**

---

## üß™ Labs Breakdown

### üß™ Lab 1 ‚Äì Direct Prompt Injection
**Goal:** Override system instructions via user-controlled input.
- **Focus:** "Ignore previous instructions", Role manipulation, Safety bypass attempts.
- **Skill Learned:** Mastering how to break the LLM instruction hierarchy.

### üß™ Lab 2 ‚Äì Indirect Prompt Injection
**Goal:** Injection via documents, emails, or hidden content.
- **Focus:** Malicious instructions inside data, Innocent-looking user prompts, Summarization abuse.
- **Skill Learned:** Understanding dangerous RAG/enterprise application risks.

### üß™ Lab 3 ‚Äì System Prompt Leakage
**Goal:** Exposing hidden system rules, policies, or configurations.
- **Focus:** Meta-requests, Debug mode tricks, Simulation-based leakage.
- **Skill Learned:** Implications of sensitive AI configuration exposure.

### üõ°Ô∏è Lab 4 ‚Äì Defense Hardening (Secure Coding)
**Goal:** Hardening a vulnerable chatbot to be secure and production-ready.
- **Includes:** `vulnerable_prompt.py`, `hardened_prompt.py`, Automated injection tests.
- **Skill Learned:** Secure coding for AI systems (Rare skill üî•).

---

## ÔøΩ Usage Examples

### 1Ô∏è‚É£ Run all tests (API Key required)
```bash
./run-tests.sh
```

### 2Ô∏è‚É£ Run free local tests (Ollama)
```bash
./run-tests-free.sh
```

### 3Ô∏è‚É£ Run a specific lab
```bash
cd labs/lab1-direct-injection
# Follow lab instructions located in the directory
```

---

## ÔøΩüîê Secure Coding & Defense Strategy

The project's [`DEFENSE.md`](./DEFENSE.md) outlines real-world mitigation strategies:

### ‚úî Secure Prompt Engineering
- Clear role definition.
- Explicit refusal of meta-instructions.
- Instruction priority enforcement.

### ‚úî Instruction vs Data Separation
```text
[INSTRUCTIONS] Trusted system rules [/INSTRUCTIONS]
[UNTRUSTED_CONTENT] User / document input [/UNTRUSTED_CONTENT]
```

### ‚úî Input Validation (Code-Level)
- Regex-based injection detection.
- Unicode obfuscation removal.
- Blocking system-level instructions from user input.

### ‚úî Least Privilege Design
- LLM tools with restricted permissions.
- No direct DB / filesystem access.
- External policy enforcement layer.

### ‚úî Output Monitoring
- System prompt leakage detection.
- PII / sensitive data patterns.
- Human-in-the-loop for risky actions.

---

## ü§ñ Automated Security Testing (Production Ready)

We use **Promptfoo** integrated with **CI/CD** for continuous security verification.

```bash
./run-tests.sh
```

## üß™ Testing on Kali Linux (FREE mode ‚Äì no API key)

This project supports **free, local testing** on Kali using **Ollama + Promptfoo** (recommended for students).

### 1Ô∏è‚É£ Install Prerequisites (Node.js + npm)
Node.js and npm are required to run Promptfoo.

**Install on Kali (simple way):**
```bash
sudo apt update
sudo apt install -y nodejs npm
node -v
npm -v
```
> **Tip:** If the Node version in the Kali repository is too old, follow the [NodeSource method](https://github.com/nodesource/distributions) to install a newer version.

### 2Ô∏è‚É£ Install Ollama (Local LLM)
On Linux/Kali, install Ollama using the official script:
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama --version
```

### 3Ô∏è‚É£ Pull a Model
Pull a model to use for testing.
> **For low-end laptops (8GB RAM):** We recommend `llama3.2` or `phi3` for faster performance.
```bash
ollama pull llama3.2
ollama list
```

### 4Ô∏è‚É£ Install Promptfoo
```bash
sudo npm i -g promptfoo@latest
promptfoo --version
```

### 5Ô∏è‚É£ Run Free Regression Tests (Ollama)
Run the tests from the repository root:
```bash
chmod +x run-tests-free.sh
./run-tests-free.sh
```
Promptfoo will automatically generate HTML & JSON reports in the `reports/` folder.

### 6Ô∏è‚É£ View the Report
```bash
ls -la reports/
# Open the report in your browser (e.g., Firefox)
# firefox reports/free-report.html
```

### üîß Troubleshooting (Common on Kali/Linux)
If Promptfoo cannot connect to Ollama on `localhost`, set the IPv4 base URL and retry:
```bash
export OLLAMA_BASE_URL="http://127.0.0.1:11434"
./run-tests-free.sh
```

**This proves:**  
üëâ Security is continuously enforced using local, offline tools‚Äîperfect for isolated lab environments.

---

## üìä Measuring Security Effectiveness

| Metric | Target |
|--------|--------|
| Injection Success Rate | < 5% |
| System Prompt Leakage | 0% |
| False Positives | < 2% |
| Detection Time | < 1 hour |

---

## üß† What This Project Demonstrates

- ‚úî **OWASP LLM01 expertise**
- ‚úî **AI Red Teaming skills**
- ‚úî **Secure coding for LLM apps**
- ‚úî **DevSecOps & CI/CD mindset**
- ‚úî **Real-world GenAI risk understanding**

*Not just ‚Äúhow to break AI‚Äù ‚Äî but how to keep it secure.*

---

## üöÄ Who This Project Is For

- **AI Security / LLM Security roles**
- **Red Team / AppSec engineers**
- **SOC analysts working with GenAI**
- **Cybersecurity students moving into AI security**

---

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üåü Show Your Support

If you like this project, please give it a ‚≠êÔ∏è!

## üìß Contact & Support

For any questions, feedback, or support, please open an issue or reach out to the author.

## üìÑ License

Distributed under the MIT License. See `LICENSE` for more information.

---

## ‚ö†Ô∏è Legal Disclaimer

This project is strictly for:
- **Education**
- **Research**
- **Authorized testing**

‚ùå **No misuse.**  
‚ùå **No production attacks.**

---

## üë§ Author

**Amresh Kumar**  
Cybersecurity | AI Red Teaming | Secure Coding  
GitHub: [Ak-cybe](https://github.com/Ak-cybe)

---

### üèÅ Final Note
> The best security engineers don‚Äôt just find vulnerabilities ‚Äî they design systems that stay secure under attack.
